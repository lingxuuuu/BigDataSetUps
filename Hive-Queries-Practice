//Enter Hive shell: 

hadoop@ubuntudev:~$ hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hadoop-ecosystem/hive-2.3.4-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-ecosystem/hadoop-2.9.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

Logging initialized using configuration in jar:file:/usr/local/hadoop-ecosystem/hive-2.3.4-bin/lib/hive-common-2.3.4.jar!/hive-log4j2.properties Async: true
Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
hive> 


//Create a db
hive> create database test_db;



//Show available dbs
hive> show databases;


//Select a db
hive> use test_db;


//Create a table: table-name: sample
hive> create table sample(id int, name string);


//Insert 1 data into the table
//hive> insert into {table-name} values({id},"{string}");
hive> insert into sample values(100,"ling");


//Insert multiple data into the table
//notice that we can use either ' ' or " " in Hive
hive> insert into sample values(101,"shu"),(102,'wei'),(103,'unknown'),(104,'JD');


//Show available tables
hive> show tables;


//Show db in the table;
hive> select * from sample;


//Show table detail
//desc formatted {table-name}; or use DESCRIBE FORMATTED table_name;
hive> desc formatted sample;
OK
# col_name            	data_type           	comment             
	 	 
id                  	int                 	                    
name                	string              	                    
	 	 
# Detailed Table Information	 	 
Database:           	test_db             	 
Owner:              	hadoop              	 
CreateTime:         	Tue Mar 16 10:46:25 PDT 2021	 
LastAccessTime:     	UNKNOWN             	 
Retention:          	0                   	 
Location:           	hdfs://localhost:9000/user/hive/warehouse/test_db.db/sample	 
Table Type:         	MANAGED_TABLE  //IMPOARTANT: Managed_table == internal_table, it is the opposite of external_table, which is not maintained by Hive       	 
Table Parameters:	 	 
	COLUMN_STATS_ACCURATE	{\"BASIC_STATS\":\"true\"}
	numFiles            	2                   
	numRows             	5                   
	rawDataSize         	39                  
	totalSize           	44                  
	transient_lastDdlTime	1615917113          
	 	 
# Storage Information	 	 
SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
Compressed:         	No                  	 
Num Buckets:        	-1                  	 
Bucket Columns:     	[]                  	 
Sort Columns:       	[]                  	 
Storage Desc Params:	 	 
	serialization.format	1                   
Time taken: 0.167 seconds, Fetched: 31 row(s)


//Create a external table
/*
CREATE EXTERNAL TABLE TableName (id int, name string)
ROW FORMAT DELIMITED   
FIELDS TERMINATED BY ',' 
LINES TERMINATED BY '\n'
STORED AS TEXTFILE
LOCATION 'place in HDFS';
*/

hive> create external table emp(id int, name string, gender string, sal int)
    row format delimited
    fields terminated by ','
    location '/user/prac/emp';
    
hive> desc formatted emp;
OK
# col_name            	data_type           	comment             
	 	 
id                  	int                 	                    
name                	string              	                    
gender              	string              	                    
sal                 	int                 	                    
	 	 
# Detailed Table Information	 	 
Database:           	test_db             	 
Owner:              	hadoop              	 
CreateTime:         	Tue Mar 16 11:49:00 PDT 2021	 
LastAccessTime:     	UNKNOWN             	 
Retention:          	0                   	 
Location:           	hdfs://localhost:9000/user/prac/emp	 
Table Type:         	EXTERNAL_TABLE      	 
Table Parameters:	 	 
	EXTERNAL            	TRUE                
	numFiles            	0                   
	totalSize           	0                   
	transient_lastDdlTime	1615920540          
	 	 
# Storage Information	 	 
SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
Compressed:         	No                  	 
Num Buckets:        	-1                  	 
Bucket Columns:     	[]                  	 
Sort Columns:       	[]                  	 
Storage Desc Params:	 	 
	field.delim         	,                   
	serialization.format	,                   
Time taken: 0.124 seconds, Fetched: 32 row(s)


//Transfer data to internal/external table
/*
i)from LFS:
  syntax:
  load data local inpath '{LFS-filepath}' into table {tablename};
ii)from HDFS:
  syntax:
  load data inpath '{HDFS filepath}' into table {tablename};
*/

hive> load data local inpath '/home/hadoop/data/emp.csv' into table emp;


//Let's create some local files to practise with HIVE

//Create some files to interact and practice HIVE 
//Using cat and put data in file1

hadoop@ubuntudev:~/data$ pwd
/home/hadoop/data

hadoop@ubuntudev:~/data$ cat > file1
Good Morning..........
Good Morning.........
Good Morning.........
Good Morning........
(Ctrl D to Exit)

hadoop@ubuntudev:~/data$ cat > file2
Hello............
Hello...........
Hello............  
Hello............
Hello...........
(Ctrl D to Exit;)

//Show data in the file. NOTICE: there is no > when you show the data. but there is > when you create the data
hadoop@ubuntudev:~/data$ cat file1
Good Morning..........
Good Morning.........
Good Morning.........
Good Morning........

//Create a new managed(internal) table and put some dummy data into the table from LFS(local file system)
hive> create table mysamp1(line string);
hive> load data local inpath '/home/hadoop/data/file1' into table mysamp1;
hive> select * from test_db.mysamp1;
OK
Good Morning..........
Good Morning.........
Good Morning.........
Good Morning........
Good Morning........

hive> te
hive> select * from mysamp1;
OK
Good Morning..........
Good Morning.........
Good Morning.........
Good Morning........
Good Morning........
Hello............
Hello...........
Hello............  
Hello............
Hello...........
Hello..........


//we just practiced how to put data into internal table, let's now practice how to interact with external table
//create a external table and put the table in '/user/prac/mysamp'
hive>create external table mysample2(line string) location '/user/prac/mysamp';

//I already have some dummy data in my HDFS
hadoop@ubuntudev:~$ hdfs dfs -cat input/emppsql/part-m-00000*
100,Ling
101,Shu
101,Wei

//put the data from HDFS to external table
//load data inpath '{HDFS filepath}' into table {tablename};
hive> load data inpath 'input/emppsql/part-m-00000' into table mysample2;
hive> select * from mysample2;
OK
100,Ling
101,Shu
101,Wei


//create table from another table 
hive> create table mysample2 as(select * from mysample1);

hive> select * from mysample2;
OK
Good Morning..........
Good Morning.........
Good Morning.........
Good Morning........
Good Morning........
Time taken: 0.249 seconds, Fetched: 5 row(s)
hive> select * from mysample1;
OK
Good Morning..........
Good Morning.........
Good Morning.........
Good Morning........
Good Morning........


// Overwrite table and Sub String 
//we have a table called tmpr1 and we are creating another table tmpr2 base on tmpr1

hive> select * from tmpr1;
OK
Hyd 2015 apr 41 
pun 2015 apr 39
ban 2015 apr 38
Hyd 2016 apr 34  
pun 2016 apr 35
ban 2016 apr 32
Hyd 2017 apr 37  
pun 2017 apr 39
ban 2017 apr 38

hive> create table tmpr2(year int,temp int);

hive> insert overwrite table tmpr2 select substr(line,5,4),substr(line,14,2) from tmpr1;  //IMPORTANT//

hive> select * from tmpr2;
OK
2015	41
2015	39
2015	38
2016	34
2016	35
2016	32
2017	37
2017	39
2017	38

hive> insert overwrite table tmpr3 select year,max(temp),min(temp) from tmpr2 group by year;

hive> select * from tmpr3;
OK
2015	41	38
2016	35	32
2017	39	37

//More practises:

hadoop@ubuntudev:~/data$ cat > tmpr1;
Hyd 2015 apr 41 
pun 2015 apr -39
ban 2015 apr 38
Hyd 2016 apr 34  
pun 2016 apr -35
ban 2016 apr 32
Hyd 2017 apr 37  
pun 2017 apr -39
(Ctrl D to exit)

hive> create table temp1(line string);

hive> load data local inpath '/home/hadoop/data/tmpr1' into table temp1;

hive> select * from temp1;
OK
Hyd 2015 apr 41 
pun 2015 apr -39
ban 2015 apr 38
Hyd 2016 apr 34  
pun 2016 apr -35
ban 2016 apr 32
Hyd 2017 apr 37  
pun 2017 apr -39
ban 2017 apr 38

hive> insert overwrite table temp2
    >      select * from (
    >      select substr(line,5,4),substr(line,14,2) from temp1 where substr(line,14,1)!='-'
    >      union all
    >      select substr(line,5,4),substr(line,14,3) from temp1 where substr(line,14,1)='-')x;
   
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Query ID = hadoop_20210318043009_799c8b5c-2ee6-4b57-84bc-2bf7e45ade5c
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1615982528081_0001, Tracking URL = http://ubuntudev:8088/proxy/application_1615982528081_0001/
Kill Command = /usr/local/hadoop-ecosystem/hadoop-2.9.2/bin/hadoop job  -kill job_1615982528081_0001
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2021-03-18 04:30:23,717 Stage-1 map = 0%,  reduce = 0%
2021-03-18 04:30:31,214 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.59 sec
MapReduce Total cumulative CPU time: 1 seconds 590 msec
Ended Job = job_1615982528081_0001
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to directory hdfs://localhost:9000/user/hive/warehouse/test_db.db/temp2/.hive-staging_hive_2021-03-18_04-30-09_058_5961382997010279705-1/-ext-10000
Loading data to table test_db.temp2
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 1.59 sec   HDFS Read: 5632 HDFS Write: 144 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 590 msec
OK
Time taken: 23.905 seconds

hive> select * from temp2;
OK
2015	41
2015	-39
2015	38
2016	34
2016	-35
2016	32
2017	37
2017	-39
2017	38

hive> insert overwrite table temp3
    >      select year,max(temp),min(temp) from temp2 group by year;

hive> select * from temp3;
OK
2015	41	-39
2016	34	-35
2017	38	-39



------------------------------------------------------------------------------------------------------------------------------------


//File formats from Hive_queries practice 2



File formats
-------------

CREATE TABLE tab_avro (field1 int, field2 string)
STORED AS AVRO;

insert into tab_avro values(1,"abc");
insert into tab_avro values(2,"xyz");
insert into tab_avro values(3,"lmn");


CREATE TABLE my_avro_table(notused INT)
  ROW FORMAT SERDE
  'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
  WITH SERDEPROPERTIES (
    'avro.schema.url'='file:///tmp/schema.avsc')
  STORED as INPUTFORMAT
  'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
  OUTPUTFORMAT
  'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat';

Create table textfile_table
(column_specs)
stored as textfile;

Create table sequencefile_table
(column_specs)
stored as sequencefile;

Create table RCfile_table
(column_specs)
stored as rcfile;

Create table avro_table
(column_specs)
stored as avro;

Create table orc_table
(column_specs)
stored as orc;

Create table parquet_table
(column_specs)
stored as parquet;

Joins
------

$ cat emps1
101,aaa,1000,m,11
102,bbb,2000,f,12
103,ccc,3000,m,12
104,ddd,4000,f,13
105,eee,5000,m,11
106,fff,6000,f,14
107,ggg,7000,m,15
108,hhh,8000,f,16


$ cat dept1
11,mrkt,hyd
12,HR,delhi
13,fin,pune
17,HR,hyd
18,fin,pune
19,mrkt,delhi



hive> create database joinsdb;

hive> use joinsdb;

hive> create table emp(id int, name string,sal int, gender string, dno int)
      row format delimited 
      fields terminated by ',';

hive> load data local inpath '/home/pylearn2018/datasets/emp1' into table emp;

hive> select * from emp;
101,aaa,1000,m,11
102,bbb,2000,f,12
103,ccc,3000,m,12
104,ddd,4000,f,13
105,eee,5000,m,11
106,fff,6000,f,14
107,ggg,7000,m,15
108,hhh,8000,f,16

hive> create table dept(dno int, dname string,city string)
      row format delimited 
      fields terminated by ',';


hive> load data local inpath '/home/pylearn2018/datasets/dept1'into table dept;

hive> select * from dept;
11,mrkt,hyd
12,HR,delhi
13,fin,pune
17,HR,hyd
18,fin,pune
19,mrkt,delhi

 

1)Inner Join:

hive> select id,name,sal,gender,e.dno,d.dno,dname,city
      from  emp e join dept d 
      on (e.dno=d.dno);
101	aaa	1000	m	11	11	mrkt	hyd
102	bbb	2000	f	12	12	HR	delhi
103	ccc	3000	m	12	12	HR	delhi
104	ddd	4000	f	13	13	fin	pune
105	eee	5000	m	11	11	mrkt	hyd



2)Left outer Join :

hive> select id,name,sal,gender,e.dno,d.dno,dname,city                       
      from  emp e left outer join dept d
      on (e.dno=d.dno);                 
101	aaa	1000	m	11	11	mrkt	hyd
102	bbb	2000	f	12	12	HR	delhi
103	ccc	3000	m	12	12	HR	delhi
104	ddd	4000	f	13	13	fin	pune
105	eee	5000	m	11	11	mrkt	hyd
106	fff	6000	f	14	NULL	NULL	NULL
107	ggg	7000	m	15	NULL	NULL	NULL
108	hhh	8000	f	16	NULL	NULL	NULL



3)Right outer Join:

hive> select id,name,sal,gender,e.dno,d.dno,dname,city                       
      from  emp e right outer join dept d
      on (e.dno=d.dno); 
101	aaa	1000	m	11	11	mrkt	hyd
105	eee	5000	m	11	11	mrkt	hyd
102	bbb	2000	f	12	12	HR	delhi
103	ccc	3000	m	12	12	HR	delhi
104	ddd	4000	f	13	13	fin	pune
NULL	NULL	NULL	NULL	NULL	17	HR	hyd
NULL	NULL	NULL	NULL	NULL	18	fin	pune
NULL	NULL	NULL	NULL	NULL	19	mrkt	delhi


Full outer Join;

hive> select id,name,sal,gender,e.dno,d.dno,dname,city                       
      from  emp e full outer join dept d
      on (e.dno=d.dno); 
105	eee	5000	m	11	11	mrkt	hyd
101	aaa	1000	m	11	11	mrkt	hyd
103	ccc	3000	m	12	12	HR	delhi
102	bbb	2000	f	12	12	HR	delhi
104	ddd	4000	f	13	13	fin	pune
106	fff	6000	f	14	NULL	NULL	NULL
107	ggg	7000	m	15	NULL	NULL	NULL
108	hhh	8000	f	16	NULL	NULL	NULL
NULL	NULL	NULL	NULL	NULL	17	HR	hyd
NULL	NULL	NULL	NULL	NULL	18	fin	pune
NULL	NULL	NULL	NULL	NULL	19	mrkt	delhi


Denormalizing:

hive> create table fullouter(id int, name string,sal int, gender string ,dno int, dname string,city string);

hive> insert overwrite table fullouter
       select id, name, sal, gender,e.dno,d.dno,dname, city
       from emp l full outer join dept r
       on (l.dno=r.dno);

hive> select * from fullouter;

105	eee	5000	m	11	11	mrkt	hyd
101	aaa	1000	m	11	11	mrkt	hyd
103	ccc	3000	m	12	12	HR	delhi
102	bbb	2000	f	12	12	HR	delhi
104	ddd	4000	f	13	13	fin	pune
106	fff	6000	f	14	NULL	NULL	NULL
107	ggg	7000	m	15	NULL	NULL	NULL
108	hhh	8000	f	16	NULL	NULL	NULL
NULL	NULL	NULL	NULL	NULL	17	HR	hyd
NULL	NULL	NULL	NULL	NULL	18	fin	pune
NULL	NULL	NULL	NULL	NULL	19	mrkt	delhi







Json Data Processing Using Hive.


$ cat > json1

{"name":"Blake","age":25,"gender":"m"}
{"name":"Ramya","age":27,"gender":"f"}
{"name":"John","age":22,"gender":"m","city":"pune"}
{"name":"Thanu","age":24,"gender":"f","city":"hyd"}


hive> create database jsonlab;

hive> use jsonlab;
OK

hive> create table jsonsamp1(line string);

hive> load data local inpath 'json1' into table jsonsamp1;

hive> select * from jsonsamp1;

{"name":"Blake","age":25,"gender":"m"}
{"name":"Ramya","age":27,"gender":"f"}
{"name":"John","age":22,"gender":"m","city":"pune"}
{"name":"Thanu","age":24,"gender":"f","city":"hyd"}

Now making this data structured

hive> select get_json_object(line,'$.name') from jsonsamp1;

Blake
Ramya
John
Thanu

hive> select                
       get_json_object(line,'$.name') ,
       get_json_object(line,'$.age') , 
       get_json_object(line,'$.gender') ,
       get_json_object(line,'$.city')  
       from jsonsamp1;

Blake	25	m	NULL
Ramya	27	f	NULL
John	22	m	pune
Thanu	24	f	hyd


    (or) another method

hive> select x.* from jsonsamp1
       lateral view json_tuple(line,'name','age','gender','city') x as n,a,s,c;

in above x.* means n,a,s,c

Now creating a table and storing this result in it

hive> create table jsonsamp2(name string, age int,gender string, city string);

hive> insert overwrite table jsonsamp2
      select x.* from jsonsamp1
       lateral view json_tuple(line,'name','age','gender','city') x as n,a,s,c;



hive> select * from jsonsamp2;  

Blake	25	m	NULL
Ramya	27	f	NULL
John	22	m	pune
Thanu	24	f	hyd                                     

  



ex:2 :nested records

$ cat json2
{"name":"Ravi","age":25,"wife":{"name":"banu","age":24},"city":"hyd"}
{"name":"Ajay","age":26,"wife":{"name":"kavitha","age":21},"city":"pune"}
{"name":"John","age":32,"wife":{"name":"sony","age":27},"city":"pune"}

hive> create table jsamp1(line string);

hive> load data local inpath 'json2' into table jsamp1;

hive> select * from jsamp1;

{"name":"Ravi","age":25,"wife":{"name":"banu","age":24},"city":"hyd"}
{"name":"Ajay","age":26,"wife":{"name":"kavitha","age":21},"city":"pune"}
{"name":"John","age":32,"wife":{"name":"sony","age":27},"city":"pune"}
Time taken: 0.574 seconds, Fetched: 3 row(s)

now making data structured

hive> create table jsamp2(name string, age int, wife string, city string);

hive> insert overwrite table jsamp2
       select x.* from jsamp1
      lateral view json_tuple(line,'name','age','wife','city') x as n,a,w,c;

hive> select * from jsamp2;
OK
Ravi	25	{"name":"banu","age":24}	hyd
Ajay	26	{"name":"kavitha","age":21}	pune
John	32	{"name":"sony","age":27}	pune
Time taken: 0.162 seconds, Fetched: 3 row(s)

still making data structured

hive> create table jsamp3(hname string, wname string,hage int, wage int, city string);

hive> insert overwrite table jsamp3
        select name, get_json_object(wife,'$.name'),
        age , get_json_object(wife,'$.age'),
       city from jsamp2;

hive> select * from jsamp3;
OK
Ravi	banu	25	24	hyd
Ajay	kavitha	26	21	pune
John	sony	32	27	pune
Time taken: 0.175 seconds, Fetched: 3 row(s)




Url Data Processing
--------------------

URL or weblogs are used for predicting user behavior


$ cat  urls
http://yoursite.com/bigdata/mongodb?name=Raj&age=25&city=hyd
http://yoursite.com/bigdata/spark?name=Swapna&city=del&desig=se
http://yoursite.com/bigdata/spark?name=Satwik&age=26&city=hyd
http://yoursite.com/oops/java?name=Manoj&gender=m&age=35

Url contains , 3 parts of information.

 i) Host name:  yoursite.com

ii) Path: /bigdata/mongodb

iii) Query string:  name=Manoj&gender=m&age=35, 

Hive hase predefined parsers , to extract basic entities of url.

 i) parse_url() ---> udf
 ii) parse_url_tuple() ---> udtf.


Ex:

hive> create database urldb;

hive> use urldb;

hive> create table urlsamp1(line string);

hive> load data local inpath 'urls' into table urlsamp1;


hive> select * from urlsamp1;

http://yoursite.com/bigdata/mongodb?name=Raj&age=25&city=hyd
http://yoursite.com/bigdata/spark?name=Swapna&city=del&desig=se
http://yoursite.com/bigdata/spark?name=Satwik&age=26&city=hyd
http://yoursite.com/oops/java?name=Manoj&gender=m&age=35

//using UDF
hive> select 
         parse_url(line,'HOST'),            
         parse_url(line,'PATH'),
         parse_url(line,'QUERY')
     from urlsamp1;

 (or)
//using UDTF
hive>select x.* from urlsamp1
      lateral view parse_url_tuple(line,'HOST','PATH','QUERY') x as h,p,q;


yoursite.com	/bigdata/mongodb        name=Raj&age=25&city=hyd
yoursite.com	/bigdata/spark	name=Swapna&city=del&desig=se
yoursite.com	/bigdata/spark	name=Satwik&age=26&city=hyd
yoursite.com	/oops/java	name=Manoj&gender=m&age=35
Time taken: 0.605 seconds, Fetched: 4 row(s)




hive> create table urlsamp2(host string, path string, query string);

hive> insert overwrite table urlsamp2
       select x.* from urlsamp1
      lateral view parse_url_tuple(line,'HOST','PATH','QUERY') x as h,p,q;

hive> select * from urlsamp2;
yoursite.com	/bigdata/mongodb        name=Raj&age=25&city=hyd
yoursite.com	/bigdata/spark	name=Swapna&city=del&desig=se
yoursite.com	/bigdata/spark	name=Satwik&age=26&city=hyd
yoursite.com	/oops/java	name=Manoj&gender=m&age=35
Time taken: 0.605 seconds, Fetched: 4 row(s)


hive> create table urlsamp3(host string, path array<string>,query map<string,string>);

hive> insert overwrite table urlsamp3
       select host, 
       split(path,'/'),
       str_to_map(query,'&','=')
       from urlsamp2;

hive> select * from urlsamp3;
OK
yoursite.com	["","bigdata","mongodb"]	{"name":"Raj","age":"25","city":"hyd"}
yoursite.com	["","bigdata","spark"]	{"name":"Swapna","city":"del","desig":"se"}
yoursite.com	["","bigdata","spark"]	{"name":"Satwik","age":"26","city":"hyd"}
yoursite.com	["","oops","java"]	{"name":"Manoj","gender":"m","age":"35"}
Time taken: 0.173 seconds, Fetched: 4 row(s)



hive> create table urlsamp4(host string,category string, course string,name string,age int,gender string,city string, desig string);
     

Time taken: 0.038 seconds
hive> insert overwrite table urlsamp4
       select host, path[1], path[2],
      query['name'], query['age'],
      query['gender'], query['city'],
      query['desig'] from  urlsamp3;

select * from urlsamp4;
OK
yoursite.com	bigdata	mongodb	Raj	25	NULL	hyd	NULL
yoursite.com	bigdata	spark	Swapna	NULL	NULL	del	se
yoursite.com	bigdata	spark	Satwik	26	NULL	hyd	NULL
yoursite.com	oops	java	Manoj	35	m	NULL	NULL
Time taken: 0.16 seconds, Fetched: 4 row(s)


--------------------------------------------
to extract all keys of a map collection/

hive> select  map_keys(query)  from urlsamp3;

["age","name","city"]
["desig","name","city"]
["age","name","city"]
["gender","age","name"]

hive> select  map_values(query) from urlsamp3;


["25","Raj","hyd"]
["se","Swapna","del"]
["26","Satwik","hyd"]
["m","35","Manoj"]



hive> create table urlsamp3_1(host string,key array<string>,value array<string>);
OK
Time taken:
hive> insert overwrite table urlsamp3_1
       select host, 
         map_keys(query), map_values(query) 
       from urlsamp3;

 select * from urlsamp3_1;
OK
yoursite.com	["name","age","city"]	["Raj","25","hyd"]
yoursite.com	["name","city","desig"]	["Swapna","del","se"]
yoursite.com	["name","age","city"]	["Satwik","26","hyd"]
yoursite.com	["name","gender","age"]	["Manoj","m","35"]
Time taken: 0.184 seconds, Fetched: 4 row(s)


gedit hivejsonscript1.hql


create database hivedb;

use hivedb;

create table jsamp1(line string);

load data local inpath 'json2' into table jsamp1;

select * from jsamp1;


For submitting or Executing the script:
open a new terminal and type this

 $ hive -f hivejsonscript1.hql

All the statements will be executing one by one
---------------------------------------------------------------------------------

open terminal a
$ hive -e 'select * from emp'


-e -------->for executing a single query
-f--------->for executing set of queries


hive> set hive.cli.print.header=true;
hive> select * from emp;

xml data processing


[training@localhost ~]$ cat xml1
<rec><name>Ravi</name><age>25</age><city>hyd</city></rec>
<rec><name>Rani</name><age>24</age><gender>f</gender></rec>
<rec><name>Sampath</name><gender>m</gender><city>Del</city></rec>



hive> create table xmlsamp1(line string);

hive> load data local inpath 'xml1'into table xmlsamp1; 


hive> select * from xmlsamp1;
OK
<rec><name>Ravi</name><age>25</age><city>hyd</city></rec>
<rec><name>Rani</name><age>24</age><gender>f</gender></rec>
<rec><name>Sampath</name><gender>m</gender><city>Del</city></rec>

hive> select count(*) from xmlsamp1;
<rec><name>Ravi</name><age>25</age><city>hyd</city></rec>
<rec><name>Rani</name><age>24</age><gender>f</gender></rec>
<rec><name>Sampath</name><gender>m</gender><city>Del</city></rec>

hive> select xpath_string(line,'rec/name') from xmlsamp1;
Ravi
Rani
Sampath

hive> select 
    >  xpath_string(line,'rec/name'),
    >  xpath_int(line,'rec/age'),
    >  xpath_string(line,'rec/gender'),
    >  xpath_string(line,'rec/city')
    > from xmlsamp1;

Ravi    25              hyd
Rani    24      f
Sampath 0       m       Del

if string fields is missed, it returns blank string, if numeric field is missed it returns 0.




_ex:2________________

xml with nested tags.

$ cat xml2
<rec><name><fname>Ravi</fname><lname>kumar</lname></name><age>25</age><contact><email><personal>ravi@gmail.com</personal><official>ravi@infy.com</official></email><phone><mobile>9988665544</mobile><office>9977665544</office><residence>9955443333</residence></phone></contact></rec>


hive> create table xmlsamp2(line string);

hive> load data local inpath 'xml2' into table xmlsamp2;

hive> select * from xmlsamp2;
OK
<rec><name><fname>Ravi</fname><lname>kumar</lname></name><age>25</age><contact><email><personal>ravi@gmail.com</personal><official>ravi@infy.com</official></email><phone><mobile>9988665544</mobile><office>9977665544</office><residence>9955443333</residence></phone></contact></rec>

hive> create table xmlsamp3(fname string,lname string,age int, personal_email string,official_email string,mobile string, office string, residence string);

hive> insert overwrite table xmlsamp3
select 
xpath_string(line,'rec/name/fname'),
xpath_string(line,'rec/name/lname'),
xpath_int(line,'rec/age'),          
xpath_string(line,'rec/contact/email/personal'),
xpath_string(line,'rec/contact/email/official'),
xpath_string(line,'rec/contact/phone/mobile'),
xpath_string(line,'rec/contact/phone/office'),
xpath_string(line,'rec/contact/phone/residence') 
from xmlsamp2;

hive> select * from xmlsamp3;
OK
Ravi    kumar   25      ravi@gmail.com  ravi@infy.com     9988665544   9977665544   9955443333



Partitioning
-------------

cat > emp
101,aaa,1000,m,11
102,bbb,2000,f,12
103,ccc,3000,m,12
104,ddd,4000,f,13
105,eee,5000,m,11

hive>create table emp(id int,name string,sal int,gender string, dno int)
row format delimited
fields terminated by ',';

hive>load data local inpath 'emps' into table emp
hive>select * from emp;
101	aaa	1000	m	11
102	bbb	2000	f	12
103	ccc	3000	m	12
104	ddd	4000	f	13
105	eee	5000	m	11




Creating Partitioned table:

create table emppart(id int,name string,sal int,gender string, dno int)
partitioned by (gen string)
row format delimited
fields terminated by ',';

here we have 5 physical columns and 1 logical column,this logical column acts as partitioning column
but backend data file contains only 5 fields
partitions are not created,whenever a table is created,these partitions(sub-directories) are created when data is loaded/inserted

hive>insert overwrite table emppart
     partition(gen='m')
     select * from emp where gender='m';


hive>insert overwrite table emppart
     partition(gen='f')
     select * from emp where gender='f';


hive>select * from emp;

It is non-partitioned, displays all


Bucketing
----------

gedit junesales
p1,1000
p2,3000
p3,5000
p4,7000
p5,9000
p1,1000
p3,5000
p2,3000
p1,1000
p4,7000
p5,9000
p3,5000
p2,3000
p1,1000
p4,7000
p5,9000
p3,5000
p2,3000
p1,1000
p4,7000
p4,7000
p5,9000
p3,5000
p2,3000
p1,1000
p4,7000
p5,9000
p1,1000
p3,5000
p2,3000
p1,1000
p4,7000

create database bucks;

use bucks;

create table junesales(pid string,price int)
row format delimited
fields terminated by ',';


load data local inpath 'junesales' into table junesales;


select * from junesales;

now making it bucketed table
for bucketing set th following property, by default it is disabled, so enable it

hive> set hive.enforce.bucketing=true;


for partitioning a table based on particular column----->we say partitioned by(colname datatype)
for bucketing a table on a particular column --------->we say clustered by(colname)

create table buckstab(pid string,price int)
clustered by(pid)
into 3 buckets;

//now loading data
hive>insert overwrite table buckstab select * from junesales;


Integrating Sqoop with Hive

sqoop import --connect jdbc:mysql://localhost/testdb
--username root -P
--table table_name --hive-import --hive-table hive_table_name

















//CTAS ( Create Table AS)



//Difference between external table and internal table(managed table):
/*
1. Hive Internal Table
Hive owns the data for the internal tables.

It is the default table in Hive. When the user creates a table in Hive without specifying it as external, then by default, an internal table gets created 
in a specific location in HDFS.

By default, an internal table will be created in a folder path similar to /user/hive/warehouse directory of HDFS. We can override the default location by 
the location property during table creation. When we load data into an internal table, then Hive moves data into the warehouse directory.

If we drop the managed table or partition, the table data and the metadata associated with that table will be deleted from the HDFS.

The TRUNCATE command only works for the internal table, and Query Result Caching that saves the results of an executed Hive query for reuse on 
subsequent queries only works for the internal table.

2. Hive External Table
Hive does not manage the data of the External table.

We create an external table for external use as when we want to use the data outside the Hive. With the EXTERNAL keyword, Hive knows that it is not managing 
the table data, so it does not move data to its warehouse directory.

External tables are stored outside the warehouse directory. They can access data stored in sources such as remote HDFS locations or Azure Storage Volumes.

Whenever we drop the external table, then only the metadata associated with the table will get deleted, the table data remains untouched by Hive.

We can create the external table by specifying the EXTERNAL keyword in the Hive create table statement.

*/


//When do use internal and external tables
/*
1. Hive Internal Table
We can use the internal table in cases:
When generating temporary tables.
When required that Hive should manage the lifecycle of the table.
And when we don’t want table data after deletion.


2. Hive External Table
We can use the external table in cases:
When we are not creating the table based on the existing table.
When required to use data outside of Hive. For example, the data files are read and processed by an existing program that does not lock the files.
When we don’t want to delete the table data completely even after DROP.
When the data should not be own by Hive.
*/


//Errors
hive> insert into emp values(10000, 'Ling', F, 30000);
FAILED: SemanticException [Error 10293]: Unable to create temp file for insert values Expression of type TOK_TABLE_OR_COL not supported in insert/values
hive> insert into table emp values(10000, 'Ling', F, 30000);
FAILED: SemanticException [Error 10293]: Unable to create temp file for insert values Expression of type TOK_TABLE_OR_COL not supported in insert/values
hive> INSERT INTO TABLE emp VALUES (1000, 'Ling', F, 10000);
FAILED: SemanticException [Error 10293]: Unable to create temp file for insert values Expression of type TOK_TABLE_OR_COL not supported in insert/values
hive> 
