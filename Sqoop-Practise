sudo apt-get install libmysql-java

sudo ln -s /usr/share/java/mysql-connector-java.jar $SQOOP_HOME/lib/my-sql-connector-java.jar

1)importing a table with primary key from RDBMS to HDFS

sqoop import \
--connect jdbc:mysql://localhost/sqoopdb \
--username root \
--password root \
--table emp \
--target-dir /user/input/emp1

//check the imported item
$ hdfs dfs -ls /user/input/emp1

//output
hadoop@ubuntudev:~$ hdfs dfs -ls /user/input/emp1
Found 5 items
-rw-r--r--   1 hadoop supergroup          0 2021-03-14 22:08 /user/input/emp1/_SUCCESS
-rw-r--r--   1 hadoop supergroup         38 2021-03-14 22:08 /user/input/emp1/part-m-00000
-rw-r--r--   1 hadoop supergroup         19 2021-03-14 22:08 /user/input/emp1/part-m-00001
-rw-r--r--   1 hadoop supergroup         19 2021-03-14 22:08 /user/input/emp1/part-m-00002
-rw-r--r--   1 hadoop supergroup         38 2021-03-14 22:08 /user/input/emp1/part-m-00003

hadoop@ubuntudev:~$ hdfs dfs -cat /user/input/emp1//part-m-00000
101,abc,10000,f,11
102,xyz,20000,f,12
hadoop@ubuntudev:~$ hdfs dfs -cat /user/input/emp1//part-m-00001
103,lmn,30000,m,12
hadoop@ubuntudev:~$ hdfs dfs -cat /user/input/emp1//part-m-00002
104,pqr,40000,f,13
hadoop@ubuntudev:~$ hdfs dfs -cat /user/input/emp1//part-m-00003
105,ghi,50000,m,11
106,stu,60000,m,11



2) With 1 mapper

sqoop import \
--connect jdbc:mysql://localhost/testdb \
--username root \
--password root \
--m 1 \
--table emp \
--target-dir /user/input/emp2

//check the imported item
$ hdfs dfs -ls /user/input/emp2

//output: because we use 1 mapper, there is only item
hadoop@ubuntudev:~$ hdfs dfs -ls /user/input/emp2
Found 2 items
-rw-r--r--   1 hadoop supergroup          0 2021-03-14 21:47 /user/input/emp2/_SUCCESS
-rw-r--r--   1 hadoop supergroup        114 2021-03-14 21:47 /user/input/emp2/part-m-00000
hadoop@ubuntudev:~$ hdfs dfs -cat /user/input/emp2/*
101,abc,10000,f,11
102,xyz,20000,f,12
103,lmn,30000,m,12
104,pqr,40000,f,13
105,ghi,50000,m,11
106,stu,60000,m,11



3) Table with no primary keys
create table emp2(id int, name char(10), sal int, gender char(1), dno int);

insert into emp2 
      select * from emp;
	  
4) Query with where condition

//select employee only gender = f
sqoop import \
--connect jdbc:mysql://localhost/sqoopdb \
--username root \
--password root \
--table emp -m 1 --where 'gender="f"' \
--target-dir /user/input/emp3

//check the result
$ hdfs dfs -ls /user/input/emp3

//output: (1 mapper)
hadoop@ubuntudev:~$ hdfs dfs -ls /user/input/emp3
Found 2 items
-rw-r--r--   1 hadoop supergroup          0 2021-03-14 22:28 /user/input/emp3/_SUCCESS
-rw-r--r--   1 hadoop supergroup         57 2021-03-14 22:28 /user/input/emp3/part-m-00000
hadoop@ubuntudev:~$ hdfs dfs -cat /user/input/emp3/*
101,abc,10000,f,11
102,xyz,20000,f,12
104,pqr,40000,f,13


//select employee only gender = m
sqoop import \
--connect jdbc:mysql://localhost/sqoopdb \
--username root \
--password root \
--table emp --where 'gender="m"' \
--target-dir /user/input/emp4

//check the result (4 mapper)
$ hdfs dfs -ls /user/input/emp4

//output 
adoop@ubuntudev:~$ hdfs dfs -ls /user/input/emp4
Found 5 items
-rw-r--r--   1 hadoop supergroup          0 2021-03-14 22:31 /user/input/emp4/_SUCCESS
-rw-r--r--   1 hadoop supergroup         19 2021-03-14 22:31 /user/input/emp4/part-m-00000
-rw-r--r--   1 hadoop supergroup          0 2021-03-14 22:31 /user/input/emp4/part-m-00001
-rw-r--r--   1 hadoop supergroup         19 2021-03-14 22:31 /user/input/emp4/part-m-00002
-rw-r--r--   1 hadoop supergroup         19 2021-03-14 22:31 /user/input/emp4/part-m-00003
hadoop@ubuntudev:~$ hdfs dfs -cat /user/input/emp4/part-m-00000
103,lmn,30000,m,12
hadoop@ubuntudev:~$ hdfs dfs -cat /user/input/emp4/part-m-00001
hadoop@ubuntudev:~$ hdfs dfs -cat /user/input/emp4/part-m-00002
105,ghi,50000,m,11
hadoop@ubuntudev:~$ hdfs dfs -cat /user/input/emp4/part-m-00003
106,stu,60000,m,11


5) Selecting multiple columns
sqoop import \
--connect jdbc:mysql://localhost/sqoopdb \
--username root \
--password root \
--table emp -m 1 --columns id,name,sal \
--target-dir /user/input/emp5

//output: 
hadoop@ubuntudev:~$ hdfs dfs -ls /user/input/emp5
Found 2 items
-rw-r--r--   1 hadoop supergroup          0 2021-03-14 22:36 /user/input/emp5/_SUCCESS
-rw-r--r--   1 hadoop supergroup         84 2021-03-14 22:36 /user/input/emp5/part-m-00000
hadoop@ubuntudev:~$ hdfs dfs -cat /user/input/emp5/*
101,abc,10000
102,xyz,20000
103,lmn,30000
104,pqr,40000
105,ghi,50000
106,stu,60000


6) Selecting multiple columns with where

sqoop import \
--connect jdbc:mysql://localhost/sqoopdb \
--username root \
--password root \
--table emp  --columns id,name,sal --where 'sal>20000' \
--target-dir /user/input/emp6

//output:
hadoop@ubuntudev:~$ hdfs dfs -ls /user/input/emp6
Found 5 items
-rw-r--r--   1 hadoop supergroup          0 2021-03-14 22:39 /user/input/emp6/_SUCCESS
-rw-r--r--   1 hadoop supergroup         14 2021-03-14 22:38 /user/input/emp6/part-m-00000
-rw-r--r--   1 hadoop supergroup         14 2021-03-14 22:38 /user/input/emp6/part-m-00001
-rw-r--r--   1 hadoop supergroup         14 2021-03-14 22:39 /user/input/emp6/part-m-00002
-rw-r--r--   1 hadoop supergroup         14 2021-03-14 22:39 /user/input/emp6/part-m-00003
hadoop@ubuntudev:~$ hdfs dfs -cat /user/input/emp6/part-m-00000
103,lmn,30000
hadoop@ubuntudev:~$ hdfs dfs -cat /user/input/emp6/part-m-00001
104,pqr,40000
hadoop@ubuntudev:~$ hdfs dfs -cat /user/input/emp6/part-m-00002
105,ghi,50000
hadoop@ubuntudev:~$ hdfs dfs -cat /user/input/emp6/part-m-00003
106,stu,60000



7) Export
cat > marks
Rohith,70,80,50
Ajith,85,65,45
Aruna,85,75,65
kamal,90,80,60
miller,75,85,95

hdfs dfs -put marks /user/input

create table testdb.stdmarks(name varchar(10),s1 int,s2 int,s3 int);

sqoop export \
--connect jdbc:mysql://localhost/testdb \
--username root \
--password root \
--table stdmarks \
--export-dir /user/output

8) If the delimiter of hdfs file is tabspace('\t')

 then use --input-fields-terminated-by '\t'

$ cat > stud1.txt
abc	95	85	35
xyz	98	55	75
lmn   100	45	69
pdf	92	92	65

hdfs dfs -put stud1.txt /user/input/temp

use sqoopdb

create table sqoopdb.student(name char(20),s1 int,s2 int,s3 int);

sqoop export \
--connect jdbc:mysql://localhost/sqoopdb \
--username root \
--password root \
--table student \
--export-dir /user/input/temp/stud1.txt \
--input-fields-terminated-by '\t'


9) Incremental load

sqoop import \
--connect jdbc:mysql://localhost/testdb \
--username root \
--password root \
--table emp -m 1 \
--target-dir /user/input/emp1 \
--incremental append \
--check-column eno \
--last-value 103


sqoop import \
--connect jdbc:mysql://localhost/testdb \
--username root \
--password root \
--table emp -m 1 \
--target-dir /user/input/emp1 \
--incremental append \
--check-column eno


10) Create a Sqoop job

sqoop job --list

//command to delete a sqoop job called my job
sqoop job --delete myjob

sqoop job --create myjob \
-- import \
--connect jdbc:mysql://localhost/sqoopdb \
--table emp -m 1 \
--target-dir /user/input/emp2

//after create, excute the sqoop job
sqoop job --exec myjob -- --username root -password root
(sqoop job --exec myjob -- --username root -P)

//to verify if it has been created
hdfs dfs -ls /user/input/emp2

//output
hadoop@ubuntudev:~$ hdfs dfs -ls /user/input/emp2
Found 2 items
-rw-r--r--   1 hadoop supergroup          0 2021-03-14 21:47 /user/input/emp2/_SUCCESS
-rw-r--r--   1 hadoop supergroup        114 2021-03-14 21:47 /user/input/emp2/part-m-00000
hadoop@ubuntudev:~$ hdfs dfs -cat /user/input/emp2/*
101,abc,10000,f,11
102,xyz,20000,f,12
103,lmn,30000,m,12
104,pqr,40000,f,13
105,ghi,50000,m,11
106,stu,60000,m,11


(### The below script will not work

sqoop job --create myjob \
--import \
--connect jdbc:mysql://localhost/sqoopdb \
--username root \
--password root \
--table emp -m 1 \
--target-dir /user/input/emp10

sqoop job --exec myjob)

